{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MARS5 TTS Server on Google Colab (GPU)\n",
        "\n",
        "This notebook runs MARS5 TTS on Colab's free GPU for fast text-to-speech generation.\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime â†’ Change runtime type â†’ GPU (T4)\n",
        "2. Run all cells\n",
        "3. Copy the ngrok URL to use in your app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchaudio librosa vocos encodec safetensors regex fastapi uvicorn pyngrok nest-asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup ngrok with your authtoken\n",
        "import os\n",
        "NGROK_AUTHTOKEN = \"35raLM4uJUBfXvR7JzUkLP9jq8Z_5vaPeepg1dQ5kaGq816w1\"  # Your ngrok authtoken\n",
        "\n",
        "print(\"Ngrok authtoken configured!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monkey-patch the trim function to handle None trim_db (avoids NumPy 2.0 issues)\n",
        "# The trim function is called from inference.py even when trim_db is None\n",
        "try:\n",
        "    import sys\n",
        "    # Patch in the trim module\n",
        "    try:\n",
        "        from mars5.trim import trim as original_trim\n",
        "        def patched_trim(audio, top_db=None):\n",
        "            if top_db is None:\n",
        "                # Skip trimming - return audio as-is with None for the second return value\n",
        "                return audio, None\n",
        "            return original_trim(audio, top_db=top_db)\n",
        "        \n",
        "        # Update the module\n",
        "        import mars5.trim as trim_module\n",
        "        trim_module.trim = patched_trim\n",
        "        print(\"âœ“ Trim function patched in mars5.trim module\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not patch mars5.trim: {e}\")\n",
        "    \n",
        "    # Also try to patch in any inference modules that import trim\n",
        "    for module_name in list(sys.modules.keys()):\n",
        "        if 'mars5' in module_name and 'inference' in module_name:\n",
        "            try:\n",
        "                module = sys.modules[module_name]\n",
        "                if hasattr(module, 'trim'):\n",
        "                    module.trim = patched_trim\n",
        "                    print(f\"âœ“ Trim function patched in {module_name}\")\n",
        "            except:\n",
        "                pass\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not patch trim function: {e}\")\n",
        "\n",
        "print(\"âœ“ MARS5 ready with trim patch!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MARS5 model\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "import wave\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import logging\n",
        "\n",
        "# Fix NumPy 2.0 compatibility issue in MARS5\n",
        "# Downgrade numpy to 1.x for compatibility with MARS5\n",
        "try:\n",
        "    import numpy as np_check\n",
        "    if np_check.__version__.startswith('2.'):\n",
        "        print(\"NumPy 2.x detected - MARS5 requires NumPy 1.x\")\n",
        "        print(\"Installing NumPy 1.26.4 for compatibility...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', '--quiet', 'numpy==1.26.4'])\n",
        "        import importlib\n",
        "        import sys\n",
        "        # Reload numpy module\n",
        "        if 'numpy' in sys.modules:\n",
        "            del sys.modules['numpy']\n",
        "        import numpy as np\n",
        "        print(f\"âœ“ NumPy downgraded to {np.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: NumPy compatibility check: {e}\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Loading MARS5 model...\")\n",
        "mars5, config_class = torch.hub.load('Camb-ai/mars5-tts', 'mars5_english', trust_repo=True)\n",
        "mars5 = mars5.cuda() if torch.cuda.is_available() else mars5\n",
        "print(f\"Model loaded on device: {next(mars5.parameters()).device}\")\n",
        "print(\"âœ“ MARS5 model ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"MARS5 TTS API\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class TTSRequest(BaseModel):\n",
        "    text: str\n",
        "    deep_clone: bool = False\n",
        "\n",
        "@app.post(\"/tts\")\n",
        "async def tts_endpoint(request: TTSRequest):\n",
        "    try:\n",
        "        if not request.text or not request.text.strip():\n",
        "            return JSONResponse(\n",
        "                status_code=400,\n",
        "                content={\"error\": \"Text is required\"}\n",
        "            )\n",
        "        \n",
        "        logger.info(f\"Generating TTS for: {request.text[:50]}...\")\n",
        "        print(f\"[TTS] Request received: {len(request.text)} chars, deep_clone={request.deep_clone}\")\n",
        "        \n",
        "        # Configure for shallow clone (low latency)\n",
        "        # Disable trim_db to avoid NumPy 2.0 compatibility issues\n",
        "        cfg = config_class(\n",
        "            deep_clone=request.deep_clone,\n",
        "            rep_penalty_window=30,\n",
        "            top_k=20,\n",
        "            temperature=0.6,\n",
        "            freq_penalty=1,\n",
        "            trim_db=None  # Disable trimming to avoid NumPy 2.0 issue\n",
        "        )\n",
        "        print(f\"[TTS] Config created: deep_clone={cfg.deep_clone}\")\n",
        "        \n",
        "        # Monkey-patch MARS5's trim function to skip when trim_db is None\n",
        "        # This avoids NumPy 2.0 compatibility issues in the trim function\n",
        "        try:\n",
        "            from mars5.trim import trim as original_trim\n",
        "            def patched_trim(audio, top_db=None):\n",
        "                if top_db is None:\n",
        "                    # Skip trimming - just return audio as-is\n",
        "                    return audio, None\n",
        "                return original_trim(audio, top_db=top_db)\n",
        "            \n",
        "            # Patch the trim function in the inference module\n",
        "            import sys\n",
        "            mars5_module = sys.modules.get('mars5.trim')\n",
        "            if mars5_module:\n",
        "                mars5_module.trim = patched_trim\n",
        "                print(\"[TTS] Trim function patched to skip when trim_db is None\")\n",
        "        except Exception as patch_error:\n",
        "            print(f\"[TTS] Warning: Could not patch trim function: {patch_error}\")\n",
        "        \n",
        "        # Use minimal reference for shallow clone\n",
        "        # Make sure reference is on the same device as the model\n",
        "        model_device = next(mars5.parameters()).device\n",
        "        dummy_ref = torch.zeros(int(mars5.sr * 0.1), device=model_device)\n",
        "        print(f\"[TTS] Reference audio created: {dummy_ref.shape}, device={dummy_ref.device}\")\n",
        "        print(f\"[TTS] Model device: {model_device}\")\n",
        "        \n",
        "        # Generate TTS\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        print(f\"[TTS] Starting generation...\")\n",
        "        \n",
        "        try:\n",
        "            ar_codes, output_audio = mars5.tts(\n",
        "                request.text,\n",
        "                dummy_ref,\n",
        "                \"\",  # Empty transcript for shallow clone\n",
        "                cfg=cfg\n",
        "            )\n",
        "            print(f\"[TTS] Generation completed, processing audio...\")\n",
        "        except Exception as gen_error:\n",
        "            print(f\"[TTS] Generation error: {gen_error}\")\n",
        "            print(f\"[TTS] Error type: {type(gen_error).__name__}\")\n",
        "            import traceback\n",
        "            print(f\"[TTS] Traceback:\\n{traceback.format_exc()}\")\n",
        "            raise\n",
        "        \n",
        "        # Convert to numpy\n",
        "        if isinstance(output_audio, torch.Tensor):\n",
        "            output_audio = output_audio.cpu().numpy()\n",
        "        print(f\"[TTS] Audio converted to numpy: shape={output_audio.shape}\")\n",
        "        \n",
        "        # Normalize\n",
        "        max_val = np.abs(output_audio).max()\n",
        "        if max_val > 1.0:\n",
        "            output_audio = output_audio / max_val\n",
        "        print(f\"[TTS] Audio normalized: max_val={max_val}\")\n",
        "        \n",
        "        # Convert to int16 PCM\n",
        "        output_audio_int16 = (output_audio * 32767).astype(np.int16)\n",
        "        print(f\"[TTS] Converted to int16: shape={output_audio_int16.shape}\")\n",
        "        \n",
        "        # Convert to WAV\n",
        "        wav_buffer = io.BytesIO()\n",
        "        with wave.open(wav_buffer, 'wb') as wav_file:\n",
        "            wav_file.setnchannels(1)\n",
        "            wav_file.setsampwidth(2)\n",
        "            wav_file.setframerate(mars5.sr)\n",
        "            wav_file.writeframes(output_audio_int16.tobytes())\n",
        "        \n",
        "        wav_bytes = wav_buffer.getvalue()\n",
        "        audio_base64 = base64.b64encode(wav_bytes).decode('utf-8')\n",
        "        \n",
        "        generation_time = time.time() - start_time\n",
        "        logger.info(f\"TTS generated in {generation_time:.2f} seconds\")\n",
        "        print(f\"[TTS] Success! Generated in {generation_time:.2f}s, audio size: {len(wav_bytes)} bytes\")\n",
        "        \n",
        "        return JSONResponse({\n",
        "            \"audio\": audio_base64,\n",
        "            \"format\": \"wav\",\n",
        "            \"sample_rate\": int(mars5.sr)\n",
        "        })\n",
        "        \n",
        "    except HTTPException:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        error_type = type(e).__name__\n",
        "        logger.error(f\"TTS generation failed: {e}\", exc_info=True)\n",
        "        print(f\"[TTS] ERROR: {error_msg}\")\n",
        "        print(f\"[TTS] Error type: {error_type}\")\n",
        "        import traceback\n",
        "        tb_str = traceback.format_exc()\n",
        "        print(f\"[TTS] Full traceback:\\n{tb_str}\")\n",
        "        \n",
        "        # Return detailed error info\n",
        "        try:\n",
        "            return JSONResponse(\n",
        "                status_code=500,\n",
        "                content={\n",
        "                    \"error\": f\"TTS generation failed: {error_msg}\",\n",
        "                    \"type\": error_type,\n",
        "                    \"traceback\": tb_str[:500]  # First 500 chars of traceback\n",
        "                }\n",
        "            )\n",
        "        except Exception as json_error:\n",
        "            # If JSON serialization fails, return simple error\n",
        "            return JSONResponse(\n",
        "                status_code=500,\n",
        "                content={\"error\": f\"TTS generation failed: {error_msg}\", \"type\": error_type}\n",
        "            )\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    device = str(next(mars5.parameters()).device)\n",
        "    return JSONResponse({\n",
        "        \"status\": \"healthy\", \n",
        "        \"device\": device,\n",
        "        \"model\": \"MARS5\",\n",
        "        \"ready\": True\n",
        "    })\n",
        "\n",
        "print(\"âœ“ API endpoints created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start server with ngrok tunnel\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()  # Required for Colab's event loop\n",
        "import socket\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "port = 8000\n",
        "\n",
        "# Check if port is already in use and kill the process\n",
        "def kill_process_on_port(port):\n",
        "    try:\n",
        "        result = subprocess.run(['lsof', '-ti', f':{port}'], capture_output=True, text=True)\n",
        "        if result.stdout.strip():\n",
        "            pid = result.stdout.strip()\n",
        "            print(f\"Port {port} is in use by PID {pid}, killing it...\")\n",
        "            subprocess.run(['kill', '-9', pid], check=False)\n",
        "            import time\n",
        "            time.sleep(1)  # Wait for port to be released\n",
        "            print(f\"âœ“ Process killed, port {port} should be free now\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not check/kill process on port {port}: {e}\")\n",
        "\n",
        "# Try to kill any existing process on port 8000\n",
        "kill_process_on_port(port)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = None\n",
        "try:\n",
        "    # Try with auth token first\n",
        "    if 'NGROK_AUTHTOKEN' in globals() and NGROK_AUTHTOKEN != \"YOUR_NGROK_AUTHTOKEN\":\n",
        "        ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "    \n",
        "    # Kill any existing ngrok tunnels on this port\n",
        "    try:\n",
        "        ngrok.disconnect_all()\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    tunnel = ngrok.connect(port)\n",
        "    # Extract the actual URL from the tunnel object\n",
        "    tunnel_str = str(tunnel)\n",
        "    if '\"' in tunnel_str:\n",
        "        public_url = tunnel_str.split('\"')[1]  # Extract URL from quotes\n",
        "    elif hasattr(tunnel, 'public_url'):\n",
        "        public_url = tunnel.public_url\n",
        "    else:\n",
        "        public_url = str(tunnel)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âœ“ Server will run on: http://localhost:{port}\")\n",
        "    print(f\"âœ“ Public URL: {public_url}\")\n",
        "    print(f\"\\nðŸ“‹ Use this URL in your app (.env file):\")\n",
        "    print(f\"   MARS5_TTS_URL={public_url}\")\n",
        "    print(f\"\\n   Or test directly:\")\n",
        "    print(f\"   {public_url}/tts\")\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"\\nâš ï¸  Keep this notebook running to keep the server active!\")\n",
        "    print(\"   Colab sessions timeout after ~90 minutes of inactivity\")\n",
        "    print(\"\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Ngrok setup failed: {e}\")\n",
        "    print(\"   You can still use localhost, but it won't be accessible from outside Colab\")\n",
        "    print(f\"   Local URL: http://localhost:{port}\")\n",
        "    public_url = f\"http://localhost:{port}\"\n",
        "\n",
        "# Run server (using nest_asyncio to allow nested event loops in Colab)\n",
        "import uvicorn\n",
        "import asyncio\n",
        "\n",
        "# Use uvicorn's async server directly\n",
        "config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\", access_log=True)\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "print(\"Starting server...\")\n",
        "print(\"Server will run and show request logs below.\")\n",
        "print(\"Press Ctrl+C or interrupt the kernel to stop.\")\n",
        "\n",
        "# Run server (this will block, but that's OK in Colab)\n",
        "try:\n",
        "    asyncio.run(server.serve())\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nServer stopped.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nServer error: {e}\")\n",
        "    print(\"If port is still in use, restart the Colab runtime: Runtime -> Restart runtime\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
